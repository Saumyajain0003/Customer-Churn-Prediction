# Customer Churn Prediction

A production-ready machine learning pipeline to predict customer churn using classification models, with a clean modular architecture ready for GitHub and team collaboration.

## ğŸ¯ Project Overview

Customer churn prediction helps businesses identify at-risk customers and implement retention strategies. This project implements a complete, end-to-end ML pipeline â€” from raw data ingestion to saved model artifacts â€” with structured logging, a CLI interface, and a reusable preprocessing pipeline.

**Key Features:**
- Modular `src/` package with clean separation of concerns
- Centralized config (`src/config.py`) â€” no hardcoded paths or hyperparameters
- Structured logging throughout the pipeline
- CLI interface via `argparse` for flexible execution
- ROC curves saved as PNG files (CI/script-safe â€” no `plt.show()`)
- Results saved to `results/` as both CSV and JSON
- Trained model pipelines saved to `models/` as `.joblib` files

## ğŸ“ Project Structure

```
customer-churn-prediction/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Churn_Modelling.csv          # Raw customer churn dataset
â”œâ”€â”€ models/                           # Saved model pipelines (.joblib)
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ eda.ipynb                     # Exploratory Data Analysis
â”‚   â”œâ”€â”€ feature_engineering.ipynb     # Feature engineering experiments
â”‚   â”œâ”€â”€ models.ipynb                  # Baseline model training
â”‚   â”œâ”€â”€ advancedmodels.ipynb          # XGBoost, LightGBM, CatBoost
â”‚   â””â”€â”€ preprocessing_pipeline.joblib # Saved preprocessing pipeline (generated)
â”œâ”€â”€ results/                          # Generated evaluation outputs
â”‚   â”œâ”€â”€ results.csv                   # Metrics comparison table
â”‚   â”œâ”€â”€ results.json                  # Metrics in JSON format
â”‚   â”œâ”€â”€ plots/                        # ROC curve PNGs
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py                   # Marks src as a Python package
â”‚   â”œâ”€â”€ config.py                     # Centralized paths & hyperparameters
â”‚   â”œâ”€â”€ data_loader.py                # Data ingestion & target detection
â”‚   â”œâ”€â”€ preprocessing.py              # Scikit-learn preprocessing pipeline
â”‚   â”œâ”€â”€ trainer.py                    # Model training & persistence
â”‚   â”œâ”€â”€ evaluation.py                 # Metrics computation & ROC plots
â”‚   â””â”€â”€ results.py                    # Results saving utilities
â”œâ”€â”€ main.py                           # CLI entry point â€” runs the full pipeline
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ .gitignore                        # Excludes cache, models, .DS_Store, etc.
â””â”€â”€ README.md
```

## ğŸ“Š Dataset

The project uses `Churn_Modelling.csv` containing 10,000 bank customer records:

| Feature | Type | Description |
|---------|------|-------------|
| CreditScore | Numeric | Customer credit score |
| Geography | Categorical | Country (France / Germany / Spain) |
| Gender | Categorical | Male / Female |
| Age | Numeric | Customer age |
| Tenure | Numeric | Years as a customer |
| Balance | Numeric | Account balance |
| NumOfProducts | Numeric | Number of bank products |
| HasCrCard | Numeric | Has credit card (0/1) |
| IsActiveMember | Numeric | Active member (0/1) |
| EstimatedSalary | Numeric | Estimated annual salary |
| **Exited** | **Target** | **Churned (1) / Retained (0)** |

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.10+
- pip

### Setup

```bash
# 1. Clone the repository
git clone <repository-url>
cd customer-churn-prediction

# 2. Create a virtual environment
python -m venv venv
source venv/bin/activate        # macOS/Linux
# venv\Scripts\activate         # Windows

# 3. Install dependencies
pip install -r requirements.txt
```

## ğŸš€ Usage

### Running the Full Pipeline (CLI)

```bash
# Default â€” uses paths from src/config.py
python main.py

# Custom paths
python main.py \
  --data-path data/Churn_Modelling.csv \
  --models-dir models \
  --results-dir results \
  --plots-dir results/plots \
  --log-level INFO
```

### CLI Arguments

| Argument | Default | Description |
|----------|---------|-------------|
| `--data-path` | `data/Churn_Modelling.csv` | Path to input CSV |
| `--models-dir` | `models/` | Directory to save model `.joblib` files |
| `--results-dir` | `results/` | Directory to save CSV/JSON results |
| `--plots-dir` | `results/plots/` | Directory to save ROC curve PNGs |
| `--log-level` | `INFO` | Logging verbosity (`DEBUG/INFO/WARNING/ERROR`) |

### Running Notebooks

```bash
jupyter notebook
```

Execute notebooks in order:
1. `eda.ipynb` â€” Explore data distributions and correlations
2. `feature_engineering.ipynb` â€” Engineer and transform features
3. `models.ipynb` â€” Train and evaluate baseline models
4. `advancedmodels.ipynb` â€” XGBoost, LightGBM, CatBoost

### Using Saved Artifacts

```python
import joblib

# Load a trained model pipeline
pipeline = joblib.load("models/logisticregression_pipeline.joblib")

# Predict on new data (raw, unprocessed DataFrame)
predictions = pipeline.predict(X_new)
probabilities = pipeline.predict_proba(X_new)[:, 1]

# Load just the preprocessing pipeline
preprocessor = joblib.load("notebooks/preprocessing_pipeline.joblib")
X_processed = preprocessor.transform(X_new)
```

## ğŸ¤– Models

### Baseline Models

| Model | Strengths | Notes |
|-------|-----------|-------|
| **Logistic Regression** | Fast, interpretable, good baseline | `liblinear` solver, class-balanced |
| **Decision Tree** | Non-linear, feature importance | `max_depth=5` to prevent overfitting |

### Pipeline Architecture

Each model is wrapped in a full sklearn `Pipeline`:
```
Raw DataFrame â†’ ColumnTransformer (scale numerics, encode categoricals) â†’ Classifier
```

This means the saved `.joblib` file handles all preprocessing automatically â€” just call `.predict(X_raw)`.

## ğŸ“ˆ Evaluation Metrics

| Metric | Description | Priority |
|--------|-------------|----------|
| **Recall** | Fraction of actual churners caught | â­ Primary |
| **F1-Score** | Harmonic mean of precision & recall | High |
| **ROC-AUC** | Discrimination ability | High |
| **Precision** | Fraction of predicted churners that are real | Medium |
| **Accuracy** | Overall correctness | Low (misleading with imbalance) |

> **Why Recall?** Missing a churner (false negative) is more costly than a false alarm. Recall is prioritized to maximize retention campaign coverage.

## ğŸ“Š Outputs

After running `python main.py`:

```
results/
â”œâ”€â”€ results.csv          # Model comparison table
â”œâ”€â”€ results.json         # Same data in JSON format
â””â”€â”€ plots/
    â”œâ”€â”€ roc_logisticregression.png
    â””â”€â”€ roc_decisiontree.png

models/
â”œâ”€â”€ logisticregression_pipeline.joblib
â””â”€â”€ decisiontree_pipeline.joblib
```

## ğŸ”® Next Steps

- [ ] Add cross-validation (StratifiedKFold)
- [ ] Hyperparameter tuning (GridSearchCV / Optuna)
- [ ] Advanced models: XGBoost, LightGBM, CatBoost (`advancedmodels.ipynb`)
- [ ] SHAP values for model explainability
- [ ] SMOTE / class-weight tuning for imbalance
- [ ] REST API with FastAPI for model serving
- [ ] Unit tests for `src/` modules
- [ ] GitHub Actions CI workflow

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/my-improvement`
3. Commit your changes: `git commit -am 'Add improvement'`
4. Push: `git push origin feature/my-improvement`
5. Open a Pull Request

## ğŸ“ License

MIT License â€” see `LICENSE` for details.

## ğŸ‘¤ Author

**Saumya Jain**
>>>>>>> e42f58a (adding more models)
